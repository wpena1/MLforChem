{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "## A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import pickle\n",
    "import gzip\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"data\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "URL = \"http://deeplearning.net/data/mnist/\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "if not (PATH / FILENAME).exists():\n",
    "        content = requests.get(URL + FILENAME).content\n",
    "        (PATH / FILENAME).open(\"wb\").write(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.imshow(x_train[54].reshape((28, 28)), cmap=\"gray\")\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self,n_inputs, n_hidden, n_outputs, activation):\n",
    "        super(DNN, self).__init__()\n",
    "        if type(activation) == nn.ReLU():\n",
    "            self.model = nn.Sequential(\n",
    "            nn.Linear(n_inputs, n_hidden), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(n_hidden, n_hidden), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_outputs))\n",
    "            \n",
    "        elif type(activation) == nn.LeakyReLU():\n",
    "            self.model = nn.Sequential(\n",
    "            nn.Linear(n_inputs, n_hidden), \n",
    "            nn.LeakyReLU(), \n",
    "            nn.Linear(n_hidden, n_hidden), \n",
    "            nn.LeakyReLU(), \n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(n_hidden, n_outputs))\n",
    "            \n",
    "        else:\n",
    "            self.model = nn.Sequential(\n",
    "            nn.Linear(n_inputs, n_hidden), \n",
    "            nn.ELU(), \n",
    "            nn.Linear(n_hidden, n_hidden), \n",
    "            nn.ELU(), \n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(n_hidden, n_outputs))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    #print(m)\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        #print(m.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=784, out_features=100, bias=True)\n",
      "ELU(alpha=1.0)\n",
      "Linear(in_features=100, out_features=100, bias=True)\n",
      "ELU(alpha=1.0)\n",
      "Linear(in_features=100, out_features=100, bias=True)\n",
      "ELU(alpha=1.0)\n",
      "Linear(in_features=100, out_features=100, bias=True)\n",
      "ELU(alpha=1.0)\n",
      "Linear(in_features=100, out_features=5, bias=True)\n",
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=100, bias=True)\n",
      "  (1): ELU(alpha=1.0)\n",
      "  (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (3): ELU(alpha=1.0)\n",
      "  (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (5): ELU(alpha=1.0)\n",
      "  (6): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (7): ELU(alpha=1.0)\n",
      "  (8): Linear(in_features=100, out_features=5, bias=True)\n",
      ")\n",
      "DNN(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=100, bias=True)\n",
      "    (1): ELU(alpha=1.0)\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): ELU(alpha=1.0)\n",
      "    (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (5): ELU(alpha=1.0)\n",
      "    (6): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (7): ELU(alpha=1.0)\n",
      "    (8): Linear(in_features=100, out_features=5, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNN(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=100, bias=True)\n",
       "    (1): ELU(alpha=1.0)\n",
       "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (3): ELU(alpha=1.0)\n",
       "    (4): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (5): ELU(alpha=1.0)\n",
       "    (6): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (7): ELU(alpha=1.0)\n",
       "    (8): Linear(in_features=100, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DNN(28*28,n_hidden=100, n_outputs=5)\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B - ELU early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28076, 784])\n",
      "torch.Size([28076])\n"
     ]
    }
   ],
   "source": [
    "x_train = np.concatenate((x_train,x_valid[:5000]),axis=0)\n",
    "y_train = np.concatenate((y_train,y_valid[:5000]),axis=0)\n",
    "x_valid = x_valid[:5000]\n",
    "y_valid = y_valid[:5000]\n",
    "\n",
    "X_train_zero_four = x_train[y_train < 5]\n",
    "y_train_zero_four = y_train[y_train < 5]\n",
    "X_valid_zero_four = x_valid[y_valid < 5]\n",
    "y_valid_zero_four = y_valid[y_valid < 5]\n",
    "\n",
    "X_train_zero_four, y_train_zero_four, X_valid_zero_four, y_valid_zero_four = map(\n",
    "    torch.tensor, (X_train_zero_four, y_train_zero_four, X_valid_zero_four, y_valid_zero_four)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28076, 784])\n",
      "torch.Size([28076])\n",
      "torch.Size([2538, 784])\n",
      "torch.Size([2538])\n"
     ]
    }
   ],
   "source": [
    "n, c = X_train_zero_four.shape\n",
    "print (X_train_zero_four.shape)\n",
    "print (y_train_zero_four.shape)\n",
    "print (X_valid_zero_four.shape)\n",
    "print (y_valid_zero_four.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)\n",
    "\n",
    "def get_data(train_ds, valid_ds, bs):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
    "        DataLoader(valid_ds, batch_size=bs * 2),\n",
    "    )\n",
    "\n",
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_early_stop(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    bs = 20\n",
    "    best_loss = np.infty\n",
    "    epochs_without_progress = 0\n",
    "    max_epochs_without_progress = 20\n",
    "    PATH = './bestmodeltorch.pth'\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            loss_batch(model, loss_func, xb, yb, opt)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(\n",
    "                *[loss_batch(model, loss_func, xb_val, yb_val) for xb_val, yb_val in valid_dl]\n",
    "            )\n",
    "            \n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "        val_accuracy = accuracy(model(X_valid_zero_four),y_valid_zero_four)\n",
    "        if epoch % 5 == 0:\n",
    "            print(\"Epoch:\", epoch,\n",
    "                  \"\\tValidation accuracy: {:.3f}%\".format(val_accuracy * 100),\n",
    "                  \"\\tLoss: {:.5f}\".format(val_loss))\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': opt.state_dict(),\n",
    "            'loss': loss_func,\n",
    "            }, PATH)\n",
    "            epochs_without_progress = 0\n",
    "        else:\n",
    "            epochs_without_progress += 1\n",
    "            if epochs_without_progress > max_epochs_without_progress:\n",
    "                print(\"Early stopping\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 20\n",
    "n_batches = int(np.ceil( n / bs))\n",
    "\n",
    "train_ds = TensorDataset(X_train_zero_four, y_train_zero_four)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "\n",
    "valid_ds = TensorDataset(X_valid_zero_four, y_valid_zero_four)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=bs * 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=784, out_features=100, bias=True)\n",
      "ELU(alpha=1.0)\n",
      "Linear(in_features=100, out_features=100, bias=True)\n",
      "ELU(alpha=1.0)\n",
      "Linear(in_features=100, out_features=100, bias=True)\n",
      "ELU(alpha=1.0)\n",
      "Linear(in_features=100, out_features=100, bias=True)\n",
      "ELU(alpha=1.0)\n",
      "Linear(in_features=100, out_features=5, bias=True)\n",
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=100, bias=True)\n",
      "  (1): ELU(alpha=1.0)\n",
      "  (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (3): ELU(alpha=1.0)\n",
      "  (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (5): ELU(alpha=1.0)\n",
      "  (6): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (7): ELU(alpha=1.0)\n",
      "  (8): Linear(in_features=100, out_features=5, bias=True)\n",
      ")\n",
      "DNN(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=100, bias=True)\n",
      "    (1): ELU(alpha=1.0)\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): ELU(alpha=1.0)\n",
      "    (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (5): ELU(alpha=1.0)\n",
      "    (6): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (7): ELU(alpha=1.0)\n",
      "    (8): Linear(in_features=100, out_features=5, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch: 0 \tValidation accuracy: 97.754% \tLoss: 0.10374\n",
      "Epoch: 5 \tValidation accuracy: 79.433% \tLoss: 0.33404\n",
      "Epoch: 10 \tValidation accuracy: 97.991% \tLoss: 0.09101\n",
      "Epoch: 15 \tValidation accuracy: 96.651% \tLoss: 0.15472\n",
      "Epoch: 20 \tValidation accuracy: 96.139% \tLoss: 0.11670\n",
      "Epoch: 25 \tValidation accuracy: 98.148% \tLoss: 0.26797\n",
      "Epoch: 30 \tValidation accuracy: 97.794% \tLoss: 0.11867\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "model = DNN(28*28,n_hidden=100, n_outputs=5)\n",
    "model.apply(init_weights)\n",
    "loss_func = F.cross_entropy\n",
    "#opt = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "opt = optim.Adam(model.parameters(),lr = 0.01)\n",
    "epochs = 100\n",
    "fit_early_stop(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "class DNNClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_hidden_layers=5, n_neurons=100, optimizer_class=opt,\n",
    "                 learning_rate=0.01, batch_size=20, activation=nn.ELU(),\n",
    "                 batch_norm_momentum=None, dropout_rate=None, random_state=None,t_dl=None,v_dl=None,loss=None):\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.n_neurons = n_neurons\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.activation = activation\n",
    "        self.batch_norm_momentum = batch_norm_momentum\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.random_state = random_state\n",
    "        self.train_dl = t_dl\n",
    "        self.valid_dl = v_dl\n",
    "        self.loss_func = loss\n",
    "        if batch_norm_momentum:\n",
    "            tmp = DNN_BatchNorm(28*28,n_hidden=self.n_neurons, n_outputs=5,activation=self.activation,mu=batch_norm_momentum)\n",
    "        elif dropout_rate:\n",
    "            tmp = DNN_DropOut(28*28,n_hidden=self.n_neurons, n_outputs=5,activation=self.activation,rate=dropout_rate)\n",
    "        else:\n",
    "            tmp = DNN(28*28,n_hidden=self.n_neurons, n_outputs=5,activation=self.activation)\n",
    "        \n",
    "        self.model = tmp\n",
    "    \n",
    "    def _dnn(self):\n",
    "        model = DNN(28*28,n_hidden=self.n_neurons, n_outputs=5)\n",
    "        model.apply(init_weights)\n",
    "        return model\n",
    "        \n",
    "    \n",
    "    def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "        loss = loss_func(model(xb), yb)\n",
    "        if opt is not None:\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "        return loss.item(), len(xb)\n",
    "\n",
    "    def get_data(train_ds, valid_ds, bs):\n",
    "        return (DataLoader(train_ds, batch_size=bs, shuffle=True), DataLoader(valid_ds, batch_size=bs * 2),)\n",
    "\n",
    "    def accuracy(out, yb):\n",
    "        preds = torch.argmax(out, dim=1)\n",
    "        return (preds == yb).float().mean()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        #print(m)\n",
    "        if type(self.model) == nn.Linear:\n",
    "            nn.init.kaiming_normal_(self.model.weight, mode='fan_out', nonlinearity='relu')\n",
    "            #print(m.weight)\n",
    "    def fit_early_stop(self, epochs, loss_func, opt, train_dl, valid_dl,path):\n",
    "        model = self.model\n",
    "        bs = self.batch_size\n",
    "        best_loss = np.infty\n",
    "        epochs_without_progress = 0\n",
    "        max_epochs_without_progress = 20\n",
    "        PATH = path\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            for xb, yb in train_dl:\n",
    "                loss_batch(model, loss_func, xb, yb, opt)\n",
    "        \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                losses, nums = zip(*[loss_batch(model, loss_func, xb_val, yb_val) for xb_val, yb_val in valid_dl])\n",
    "            \n",
    "            val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "            val_accuracy = accuracy(model(X_valid_zero_four),y_valid_zero_four)\n",
    "            if epoch % 5 == 0:\n",
    "                print(\"Epoch:\", epoch,\n",
    "                  \"\\tValidation accuracy: {:.3f}%\".format(val_accuracy * 100),\n",
    "                  \"\\tLoss: {:.5f}\".format(val_loss))\n",
    "\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': opt.state_dict(),\n",
    "                'loss': loss_func,\n",
    "                }, PATH)\n",
    "                epochs_without_progress = 0\n",
    "            else:\n",
    "                epochs_without_progress += 1\n",
    "                if epochs_without_progress > max_epochs_without_progress:\n",
    "                    print(\"Early stopping\")\n",
    "                    break\n",
    "    def set_dataloader(self,train_dl,val_dl):\n",
    "        self.train_dl = train_dl\n",
    "        self.valid_dl = val_dl\n",
    "        \n",
    "    def fit(self, X, y, n_epochs=100, X_valid=None, y_valid=None):\n",
    "        n_inputs = X.shape[1]\n",
    "        self.classes_ = np.unique(y)\n",
    "        n_outputs = len(self.classes_)\n",
    "        model = self.model\n",
    "        self.optimizer_class = optim.Adam(model.parameters(),lr = 0.01)\n",
    "        bs = self.batch_size\n",
    "        best_loss = np.infty\n",
    "        epochs_without_progress = 0\n",
    "        max_epochs_without_progress = 5\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            rnd_idx = np.random.permutation(len(X))\n",
    "            for rnd_indices in np.array_split(rnd_idx, len(X_train_zero_four) // bs):\n",
    "                xb, yb = X[rnd_indices], y[rnd_indices]\n",
    "                loss_batch(model, self.loss_func, xb, yb, self.optimizer_class)\n",
    "            if X_valid is not None and y_valid is not None:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    losses, nums = zip(*[loss_batch(model, loss_func, xb_val, yb_val) for xb_val, yb_val in self.valid_dl])\n",
    "                val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "                val_accuracy = accuracy(model(X_valid_zero_four),y_valid_zero_four)\n",
    "                if epoch % 5 == 0:\n",
    "                    print(\"Epoch:\", epoch,\n",
    "                          \"\\tValidation accuracy: {:.3f}%\".format(val_accuracy * 100),\n",
    "                          \"\\tLoss: {:.5f}\".format(val_loss))\n",
    "                if val_loss < best_loss:\n",
    "                    best_loss = val_loss\n",
    "                    torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': opt.state_dict(),\n",
    "                    'loss': loss_func,\n",
    "                    }, \"./test.pth\")\n",
    "                    epochs_without_progress = 0\n",
    "                else:\n",
    "                    epochs_without_progress += 1\n",
    "                    if epochs_without_progress > max_epochs_without_progress:\n",
    "                        print(\"Early stopping\")\n",
    "                        break\n",
    "            else:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    losses, nums = zip(*[loss_batch(model, loss_func, xb_val, yb_val) for xb_val, yb_val in self.train_dl])\n",
    "                train_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "                train_accuracy = accuracy(model(X),y)\n",
    "                if epoch % 5 == 0:\n",
    "                    print(\"{}\\tLast training batch loss: {:.6f}\\tAccuracy: {:.2f}%\".format(epoch, train_loss, train_accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_clf = DNNClassifier(random_state=42,activation=nn.LeakyReLU())\n",
    "dnn_clf.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tValidation accuracy: 96.375% \tLoss: 0.13574\n",
      "Epoch: 5 \tValidation accuracy: 98.503% \tLoss: 0.06506\n",
      "Epoch: 10 \tValidation accuracy: 97.124% \tLoss: 0.08364\n",
      "Epoch: 15 \tValidation accuracy: 96.730% \tLoss: 0.14521\n",
      "Epoch: 20 \tValidation accuracy: 98.739% \tLoss: 0.07402\n",
      "Epoch: 25 \tValidation accuracy: 95.587% \tLoss: 0.25071\n",
      "Epoch: 30 \tValidation accuracy: 67.888% \tLoss: 0.73054\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "loss_func = F.cross_entropy\n",
    "#opt = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "opt = optim.Adam(dnn_clf.model.parameters(),lr = 0.01)\n",
    "epochs = 100\n",
    "dnn_clf.fit_early_stop(epochs, loss_func, opt, train_dl, valid_dl,'./vannilla.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tValidation accuracy: 95.942% \tLoss: 0.12250\n",
      "Epoch: 5 \tValidation accuracy: 98.306% \tLoss: 0.06976\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "test = DNNClassifier(random_state=42,t_dl=train_dl,v_dl=valid_dl,loss=loss_func)\n",
    "test.fit(X_train_zero_four, y_train_zero_four, n_epochs=100,X_valid=X_valid_zero_four, y_valid=y_valid_zero_four)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C - cross validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_neurons\": [50, 80, 120],\n",
    "    \"batch_size\": [10, 50, 100],\n",
    "    \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "    \"activation\": [nn.ReLU(), nn.ELU(), nn.LeakyReLU()],\n",
    "}\n",
    "rnd_search = RandomizedSearchCV(DNNClassifier(random_state=42,t_dl=train_dl,v_dl=valid_dl,loss=loss_func),\n",
    "                                param_distribs, n_iter=30,\n",
    "                                cv=3, random_state=42, verbose=2)\n",
    "\n",
    "rnd_search.fit(X_train_zero_four, y_train_zero_four, n_epochs=100,X_valid=X_valid_zero_four, y_valid=y_valid_zero_four)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DNN_BatchNorm(nn.Module):\n",
    "    def __init__(self,n_inputs, n_hidden, n_outputs, activation,mu):\n",
    "        super(DNN_BatchNorm, self).__init__()\n",
    "        if isinstance(activation,type(nn.ReLU())):\n",
    "            print(\"ReLU\")\n",
    "            self.model = nn.Sequential(\n",
    "            nn.Linear(n_inputs, n_hidden),\n",
    "            nn.BatchNorm1d(n_hidden,momentum=mu), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.BatchNorm1d(n_hidden,momentum=mu),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.BatchNorm1d(n_hidden,momentum=mu), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.BatchNorm1d(n_hidden,momentum=mu), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_outputs))\n",
    "            \n",
    "        elif isinstance(activation,type(nn.LeakyReLU())):\n",
    "            print(\"leaky\")\n",
    "            self.model = nn.Sequential(\n",
    "            nn.Linear(n_inputs, n_hidden), \n",
    "            nn.BatchNorm1d(n_hidden,momentum=mu),\n",
    "            nn.LeakyReLU(), \n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.BatchNorm1d(n_hidden,momentum=mu),\n",
    "            nn.LeakyReLU(), \n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.BatchNorm1d(n_hidden,momentum=mu),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.BatchNorm1d(n_hidden,momentum=mu),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(n_hidden, n_outputs))\n",
    "            \n",
    "        else:\n",
    "            print(\"elu\")\n",
    "            self.model = nn.Sequential(\n",
    "            nn.Linear(n_inputs, n_hidden),\n",
    "            nn.BatchNorm1d(n_hidden,momentum=mu),\n",
    "            nn.ELU(), \n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.BatchNorm1d(n_hidden,momentum=mu),\n",
    "            nn.ELU(), \n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.BatchNorm1d(n_hidden,momentum=mu),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.BatchNorm1d(n_hidden,momentum=mu),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(n_hidden, n_outputs))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DNN_DropOut(nn.Module):\n",
    "    def __init__(self,n_inputs, n_hidden, n_outputs, activation,rate):\n",
    "        super(DNN_DropOut, self).__init__()\n",
    "        self.rate = rate\n",
    "        if isinstance(activation,type(nn.ReLU())):\n",
    "            print(\"relu\")\n",
    "            self.model = nn.Sequential(\n",
    "            nn.Linear(n_inputs, n_hidden), \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=rate),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=rate),\n",
    "            nn.Linear(n_hidden, n_hidden), \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=rate),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=rate),\n",
    "            nn.Linear(n_hidden, n_outputs))\n",
    "            \n",
    "        elif isinstance(activation,type(nn.LeakyReLU())):\n",
    "            print(\"leaky relu\")\n",
    "            self.model = nn.Sequential(\n",
    "            nn.Linear(n_inputs, n_hidden), \n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=rate),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=rate),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=rate),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=rate),\n",
    "            nn.Linear(n_hidden, n_outputs))\n",
    "            \n",
    "        else:\n",
    "            print(\"elu\")\n",
    "            self.model = nn.Sequential(\n",
    "            nn.Linear(n_inputs, n_hidden),\n",
    "            nn.ELU(), \n",
    "            nn.Dropout(p=rate),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(p=rate),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(p=rate),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(p=rate),\n",
    "            nn.Linear(n_hidden, n_outputs))\n",
    "        #self.dropout = nn.Dropout(p=self.rate)\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D - Batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tValidation accuracy: 98.069% \tLoss: 0.07346\n",
      "Epoch: 5 \tValidation accuracy: 99.015% \tLoss: 0.02584\n",
      "Epoch: 10 \tValidation accuracy: 99.488% \tLoss: 0.01801\n",
      "Epoch: 15 \tValidation accuracy: 99.921% \tLoss: 0.00473\n",
      "Epoch: 20 \tValidation accuracy: 99.606% \tLoss: 0.01003\n",
      "Epoch: 25 \tValidation accuracy: 99.448% \tLoss: 0.01613\n",
      "Epoch: 30 \tValidation accuracy: 99.685% \tLoss: 0.00824\n",
      "Epoch: 35 \tValidation accuracy: 99.921% \tLoss: 0.00223\n",
      "Epoch: 40 \tValidation accuracy: 99.882% \tLoss: 0.00331\n",
      "Epoch: 45 \tValidation accuracy: 99.961% \tLoss: 0.00306\n",
      "Epoch: 50 \tValidation accuracy: 100.000% \tLoss: 0.00115\n",
      "Epoch: 55 \tValidation accuracy: 99.567% \tLoss: 0.00920\n",
      "Epoch: 60 \tValidation accuracy: 99.645% \tLoss: 0.00805\n",
      "Epoch: 65 \tValidation accuracy: 99.842% \tLoss: 0.00620\n",
      "Epoch: 70 \tValidation accuracy: 99.842% \tLoss: 0.00299\n",
      "Epoch: 75 \tValidation accuracy: 99.488% \tLoss: 0.01661\n",
      "Epoch: 80 \tValidation accuracy: 99.842% \tLoss: 0.00457\n",
      "Epoch: 85 \tValidation accuracy: 100.000% \tLoss: 0.00057\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "dnn_clf = DNNClassifier(random_state=42,activation=nn.LeakyReLU(),batch_norm_momentum=0.9)\n",
    "dnn_clf.init_weights()\n",
    "loss_func = F.cross_entropy\n",
    "#opt = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "opt = optim.Adam(dnn_clf.model.parameters(),lr = 0.01)\n",
    "epochs = 100\n",
    "dnn_clf.fit_early_stop(epochs, loss_func, opt, train_dl, valid_dl,'./batchnorm.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E - Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relu\n",
      "Epoch: 0 \tValidation accuracy: 91.411% \tLoss: 0.23804\n",
      "Epoch: 5 \tValidation accuracy: 88.534% \tLoss: 0.30492\n",
      "Epoch: 10 \tValidation accuracy: 90.071% \tLoss: 0.31291\n",
      "Epoch: 15 \tValidation accuracy: 92.947% \tLoss: 0.26837\n",
      "Epoch: 20 \tValidation accuracy: 94.287% \tLoss: 0.20364\n",
      "Epoch: 25 \tValidation accuracy: 92.356% \tLoss: 0.23833\n",
      "Epoch: 30 \tValidation accuracy: 92.632% \tLoss: 0.26522\n",
      "Epoch: 35 \tValidation accuracy: 94.050% \tLoss: 0.24215\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "dnn_clfe = DNNClassifier(random_state=42,activation=nn.ReLU(),dropout_rate=0.50)\n",
    "dnn_clfe.init_weights()\n",
    "loss_func = F.cross_entropy\n",
    "#opt = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "opt = optim.Adam(dnn_clfe.model.parameters(),lr = 0.01)\n",
    "epochs = 100\n",
    "dnn_clfe.fit_early_stop(epochs, loss_func, opt, train_dl, valid_dl,'./dropout.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leaky relu\n",
      "Epoch: 0 \tValidation accuracy: 86.840% \tLoss: 0.32394\n",
      "Epoch: 5 \tValidation accuracy: 79.472% \tLoss: 0.63043\n",
      "Epoch: 10 \tValidation accuracy: 67.652% \tLoss: 0.83198\n",
      "Epoch: 15 \tValidation accuracy: 50.473% \tLoss: 1.04839\n",
      "Epoch: 20 \tValidation accuracy: 49.882% \tLoss: 1.05426\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "dnn_clfe = DNNClassifier(random_state=42,activation=nn.LeakyReLU(),dropout_rate=0.50)\n",
    "#dnn_clfe.init_weights()\n",
    "loss_func = F.cross_entropy\n",
    "#opt = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "opt = optim.Adam(dnn_clfe.model.parameters(),lr = 0.01)\n",
    "epochs = 100\n",
    "dnn_clfe.fit_early_stop(epochs, loss_func, opt, train_dl, valid_dl,'./dropout2.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rdkitenv",
   "language": "python",
   "name": "rdkitenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
